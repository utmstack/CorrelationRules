# Logstash filter, version 4.0.2
# Fields based on https://www.elastic.co/guide/en/beats/filebeat/7.13/filebeat-module-logstash.html
# and filebeat fields.yml version 7.13.4 oss
# Support logs from logstash 7.14.1 ++
# Filter Input requirements -> fileset: datatype
#                              log: plain text
# 1. Parsing the json from beats
# 2. Parsing the message field containing the logstash log
pipeline:
  - dataTypes:
      - logstash
    steps:
      - json:
          source: raw
      - rename:
          from:
            - log.url
          to: origin.url
      - rename:
          from:
            - log.log.file.path
          to: origin.file
      - rename:
          from:
            - log.host.ip
          to: log.local.ips
      - rename:
          from:
            - log.host.mac
          to: log.local.macs
      - rename:
          from:
            - log.host.hostname
          to: origin.host
      - rename:
          from:
            - log.event.dataset
          to: action
      - rename:
          from:
            - log.agent.version
          to: log.agentVersion
      - rename:
          from:
            - log.host.os.kernel
          to: log.osVersion
      - rename:
          from:
            - log.host.os.type
          to: log.osType
      - rename:
          from:
            - log.host.architecture
          to: log.cpuArchitecture
      - cast:
          to: '[]string'
          fields:
            - log.local.ips
      - cast:
          to: '[]string'
          fields:
            - log.local.macs
      # Parsing common log parts
      - grok:
          patterns:
            - fieldName: deviceTime
              pattern: '\[{{.data}}\]'
            - fieldName: log.level
              pattern: '\[{{.data}}\]'
            - fieldName: log.component
              pattern: '\[{{.data}}\]'
            - fieldName: log.pipelineName
              pattern: '^(\[{{.data}}\])'
            - fieldName: log.msg
              pattern: '{{.greedy}}'
          source: log.message
      # Parsing when pipeline is not present
      - grok:
          patterns:
            - fieldName: deviceTime
              pattern: '\[{{.data}}\]'
            - fieldName: log.level
              pattern: '\[{{.data}}\]'
            - fieldName: log.component
              pattern: '\[{{.data}}\]'
            - fieldName: log.msg
              pattern: '{{.greedy}}'
          source: log.message
      - trim:
          function: prefix
          substring: '['
          fields:
            - deviceTime
            - log.level
            - log.component
            - log.pipelineName
      - trim:
          function: suffix
          substring: ']'
          fields:
            - deviceTime
            - log.level
            - log.component
            - log.pipelineName
      - reformat:
          fields:
            - deviceTime
          function: time
          fromFormat: '2024-07-31T17:02:07,154'
          toFormat: '2024-09-23T15:57:40.338364445Z'
      # Adding severity based on log.level
      - add:
          function: 'string'
          params:
            key: severity
            value: 'high'
          where: safe("log.level", "") && (log.level == "CRITICAL" || log.level == "FATAL" || log.level == "ERROR")
      - add:
          function: 'string'
          params:
            key: severity
            value: 'medium'
          where: safe("log.level", "") && log.level == "WARN"
      - add:
          function: 'string'
          params:
            key: severity
            value: 'low'
          where: safe("log.level", "") && (log.level == "INFO" || log.level == "DEBUG" || log.level == "TRACE")
      # Removing unused fields
      - delete:
          fields:
            - log.service
            - log.metadata
            - log.agent
            - log.host
            - log.event
            - log.ecs
            - log.log
            - log.fileset